---
title: "Homework 4"
author: "Goutham Selvakumar"
date: '2022-05-18'
output:
  pdf_document: default
  html_document: default
---
### PROBLEM 1

a. Load the two provided wine quality datasets and prepare them by (1) ensuring that all the variables have the right type (e.g., what is numeric vs. factor), (2) adding a type column to each that indicates if it is red or white wine and (2) merging the two tables together into one table (hint: try full_join()).You now have one table that contains the data on red and white wine, with a column that tells if the wine was from the red or white set (the type column you made).
```{r}
library(caret)
library(dplyr)
#Importing the datasets and separating them with a semicolon
winequality_white <- read.csv("winequality-white.csv", sep = ";")
winequality_red <- read.csv("winequality-red.csv", sep = ";")
summary(winequality_white)
summary(winequality_red)
typeof(winequality_red$chlorides)
typeof(winequality_red$citric.acid)
typeof(winequality_red$residual.sugar)
typeof(winequality_red$alcohol)
#Add the column for type of wine
winequality_red$type <- c('red')
winequality_white$type <- c('white')
#Now combine both tables using the full_join
wines <- full_join(winequality_red, winequality_white)
head(wines)
```

b. Use PCA to create a projection of the data to 2D and show a scatterplot with color showing the wine
type.
```{r}
#Create the Dummies for the dataset
dummy <- dummyVars(type ~ ., data = wines)
dummies <- as.data.frame(predict(dummy, newdata = wines))
set.seed(123)
#Calculate PCA
pca = prcomp(dummies)
#Save as data frame
rotated_data = as.data.frame(pca$x)
#Add the original label 'type' as a reference
rotated_data$Color <- wines$type
#Plot and color the labels based on wine type red (or) white
ggplot(data = rotated_data, aes(x = PC1, y = PC2, col = Color)) + geom_point(alpha = 0.3)
```

c. We are going to try kNN, SVM and decision trees on this data. Based on the ‘shape’ of the data in the visualization from (b), which do you think will do best and why?
```{r}
set.seed(123)
#Scaling is crucial for KNN
ctrl <- trainControl(method = "cv", number = 10)
knnFit <- train(type ~ ., data = wines,
                method = "knn",
                trControl = ctrl,
                preProcess = c("center", "scale"))
#Output for KNN fit
knnFit
#Fit the Model
svm1 <- train(type ~., data = wines, method = "svmLinear")
#Evaluate the Fit
svm1
#Evaluation Method
train_control = trainControl(method = "cv", number = 10)
#Fit the Model
tree1 <- train(type ~., data = wines, method = "rpart", trControl = train_control)
tree1
```

d. Use kNN (tune k), use decision trees (basic rpart method is fine), and SVM (tune C) to predict type from the rest of the variables. Compare the accuracy values – is this what you expected? Can you explain it? Note: you will need to fix the columns names for rpart because it is not able to handle the underscores. This code will do the trick (assuming you called your data wine_quality):colnames(wine_quality) <- make.names(colnames(wine_quality))
```{r}
set.seed(123)
ctrl <- trainControl(method ="cv", number = 10)
knnFit <- train(type ~., data = wines,
                method = "knn",
                trControl = ctrl,
                preProcess = c("center", "scale"),
                tuneLength = 15)
knnFit
fit <- kmeans(dummies, centers = 7, nstart = 25)
#Display the kmeans object information
fit
#I decided to use the Grid Search here to try different values of C
grid <- expand.grid(C = 10^seq(-5,2,0.5))
#Fit the Model
svm_grid <- train(type ~., data = wines, method = "svmLinear",
                  trControl = train_control, tuneGrid = grid)
#View grid search result
svm_grid
#Evaluation Method
train_control = trainControl(method = "cv", number = 10)
#Fit the Model
tree1 <- train(type ~., data = wines, method = "rpart", trControl = train_control)
#Evaluate the Fit
tree1
```

e. Use the same already computed PCA again to show a scatter plot of the data and to visualize the labels for kNN, decision tree and SVM. Note that you do not need to recreate the PCA projection, you have already done this in 1b. Here, you just make a new visualization for each classifier using its labels for color (same points but change the color). Map the color results to the classifier, that is use the “predict” function to predict the class of your data, add it to your data frame and use it as a color. This is done for KNN in the tutorial, it should be similar for the others. Consider and explain the differences in how these classifiers performed.
```{r}
#Plot and color the labels based on Wine type red (or) white
ggplot(data = rotated_data, aes(x = PC1, y = PC2, col = Color)) + geom_point(alpha = 0.3)
#Assign clusters as a new column
rotated_data$Clusters = as.factor(fit$cluster)
#Plot and color by labels
ggplot(data = rotated_data, aes(x = PC1, y = PC2 , col = Clusters)) + geom_point()
fit2 <- kmeans(dummies, centers = 2, nstart = 25)
#Assign clusters as a new column
rotated_data$Clusters = as.factor(fit2$cluster)
#Plot and color by the labels
ggplot(data = rotated_data, aes(x = PC1, y = PC2, col = Clusters)) + geom_point()
```

### PROBLEM 2

a. Load the data from the tidyverse library with the data(“Sacramento”) command and you should have a variable Sacramento. Because we have categoricals, convert them to dummy variables.
```{r}
data("Sacramento")
#Remove the Zipcode, and lon for simplicity
cpsacramento <- Sacramento %>% select(-c("latitude", "longitude", "zip"))
#Type is the largest variable to predict
dummy <- dummyVars(type ~., data = cpsacramento)
dummies <- as.data.frame(predict(dummy, newdata = cpsacramento))
head(dummies)
```

c. Use kNN to classify this data with type as the label. Tune the choice of k plus the type of distance function. Report your results – what values for these parameters were tried, which were chosen, and how did they perform with accuracy?
```{r}
#Move the Type back to the dataset
sacramento_dummies <- dummies
sacramento_dummies$type <- Sacramento$type
library(kknn)
#Setup a tuneGrid with the tuning parameters
tuneGrid <- expand.grid(kmax = 3:7,                      # Test a range of k values 3 to 7
                        kernel = c("rectangular","cos"), # Regular and cosine-based distance funtions
                        distance = 1:3)                  # Powers of Minkowski 1 to 3
#Tune and fit the model with 10-fold cross validation,
#Standardization, and our specialized tune grid
kknn_fit <- train(type ~.,
                  data = sacramento_dummies,
                  method = 'kknn',
                  trControl = ctrl,
                  preProcess = c('center', 'scale'),
                  tuneGrid = tuneGrid)
#Printing trained model provides report
kknn_fit
```

### PROBLEM 3

a. Use k-means to cluster the data. Show your usage of silhouette and the elbow method to pick the best number of clusters. Make sure it is using multiple restarts.
```{r}
library(NbClust)
library(factoextra)
#Copy wines dataset and remove type
cpwines <- wines
cpwines <- cpwines %>% select(-c("type"))
df <- cpwines
#Set seed
set.seed(123)
#Center scale allows us to standardize the data
preproc <- preProcess(df, method = c("center", "scale"))
#We have to call predict to fit our data based on preprocessing
predictors <- predict(preproc, df)
#Find the knee
fviz_nbclust(predictors, kmeans, method = "wss")
fviz_nbclust(predictors, kmeans, method = "silhouette")
#Fit the Data
fit <- kmeans(predictors, centers = 4, nstart = 25)
#Display the k means object information
fit
```

b. Use hierarchical agglomerative clustering (HAC) to cluster the data. Try at least 2 distance functions and at least 2 linkage functions (cluster distance functions), for a total of 4 parameter combinations. For each parameter combination, perform the clustering.
```{r}
dist_mat <- dist(predictors, method = 'euclidean')
#Determine assembly/agglomeration method and run hclust
hfit1 <- hclust(dist_mat, method = 'complete')
hfit1
dist_mat <- dist(predictors, method = 'euclidean')
#Determine assembly/agglomeration method and run hclust
hfit2 <- hclust(dist_mat, method = 'average')
hfit2
dist_mat <- dist(predictors, method = 'manhattan')
#Determine assembly/agglomeration method and run hclust (average uses mean)
hfit3 <- hclust(dist_mat, method = 'complete')
hfit3
dist_mat <- dist(predictors, method = 'manhattan')
#Determine assembly/agglomeration method and run hclust (average uses mean)
hfit4 <- hclust(dist_mat, method = 'average')
hfit4
#Build the new model
h1 <- cutree(hfit1, k=4)
h2 <- cutree(hfit2, k=4)
h3 <- cutree(hfit3, k=4)
h4 <- cutree(hfit4, k=4)
```

c. Compare the k-means and HAC clusterings by creating a crosstabulation between their labels.
```{r}
#Redefining the fit (I think I missed it previously)
fit <- kmeans(predictors, centers = 4, nstart = 25)
#Create a dataframe for the results
result1 <- data.frame(WineType = wines$type, HAC1 = h1, Kmeans = fit$cluster)
result2 <- data.frame(WineType = wines$type, HAC2 = h2, Kmeans = fit$cluster)
result3 <- data.frame(WineType = wines$type, HAC3 = h3, Kmeans = fit$cluster)
result4 <- data.frame(WineType = wines$type, HAC4 = h4, Kmeans = fit$cluster)
#Crosstab for HAC
result1 %>% group_by(HAC1) %>% select(HAC1, WineType) %>% table()
result2 %>% group_by(HAC2) %>% select(HAC2, WineType) %>% table()
result3 %>% group_by(HAC3) %>% select(HAC3, WineType) %>% table()
result4 %>% group_by(HAC4) %>% select(HAC4, WineType) %>% table()
#Crosstab for K Means
result <- data.frame(Type = wines$type, Kmeans = fit$cluster)
result %>% group_by(Kmeans) %>% select(Kmeans, Type) %>% table()
```

d. For comparison – use PCA to visualize the data in a scatterplot. Create 3 separate plots: use the color of the points to show (1) the type label, (2) the k-means cluster labels and (3) the HAC cluster labels.
```{r}
#Recreating the PCA scatter plot
#Create Dummies
dummy <- dummyVars(type ~ ., data = wines)
dummies <- as.data.frame(predict(dummy, newdata = wines))
set.seed(123)
#Calculate PCA
pca = prcomp(dummies)
#Save as data frame
rotated_data = as.data.frame(pca$x)
#Add original label 'type' as a reference
rotated_data$Color <- wines$type
#Plot and color the labels based on wine type red (or) white
ggplot(data = rotated_data, aes(x = PC1, y = PC2, col = Color)) + geom_point(alpha = 0.3)
rotated_data$Clusters = as.factor(h3)
#Plot and color by labels
ggplot(data = rotated_data, aes(x = PC1, y = PC2, col = Clusters)) + geom_point()
rotated_data$Clusters = as.factor(fit$cluster)
#Plot and color by labels
ggplot(data = rotated_data, aes(x = PC1, y = PC2, col = Clusters)) + geom_point()
```

### PROBLEM 4

a. Use hierarchical agglomerative clustering to cluster the Starwars data. This time we can leave the categorical variables in place, because we will use the gower metric from daisy in the cluster library to get the distances. Use average linkage. Determine the best number of clusters.
```{r}
data("starwars")
#Copy Starwars
cpstarwars <- starwars
#Remove some columns
cpstarwars <- cpstarwars %>% select(-c("name", "vehicles", "starships", "films"))
#Remove NAs
cpstarwars <- na.omit(cpstarwars)
summary(cpstarwars)
library(cluster)
#Pass dataframe directly with mertic = gower
dist_mat <- daisy(dummies, metric = "gower")
#Center scale allows us to standardize the data
preproc <- predict(preproc, cpstarwars)
#Silhouette score comparison to find K
fviz_nbclust(predictors, FUN = hcut, method = "silhouette")
#Determine the assembly/agglomeration method and run hclust
hfit <- hclust(dist_mat, method = 'average')
#Build the new model
h2 <- cutree(hfit, k=2)
summary(h2)
```

b. Produce the dendogram for (a). How might an anomaly show up in a dendogram? Do you see a Starwars character who does not seem to fit in easily? What is the advantage of considering anomalies this way as opposed to looking for unusual values relative to the mean and standard deviations, as we considered earlier in the course? Disadvantages?
```{r}
hfit <- hclust(dist_mat, method = 'average')
plot(hfit)
```

c. Use dummy variables to make this data fully numeric and then use k-means to cluster. Choose the best number of clusters.
```{r}
#Create dummy variables
dummy <- dummyVars(gender ~ ., data = cpstarwars)
dummies <- as.data.frame(predict(dummy, newdata = cpstarwars))
head(dummies)
#Create a predictors file using dummies
predictors <- dummies
#Set seed
set.seed(123)
#Center scale allows us to standardize the data
preproc <- preProcess(predictors, method = c("center", "scale"))
#We have to call predict to fit our data based on preprocessing
predictors <- predict(preproc, predictors)
#Find the knee
fviz_nbclust(predictors, kmeans, method = "wss")
fviz_nbclust(predictors, kmeans, method = "silhouette")
#Fit the data
fit <- kmeans(predictors, centers = 2, nstart = 25)
#Display the kmeans object information
fit
```

d. Compare the HAC and k-means clusterings with a crosstabulation.
```{r}
#Create a dataframe for results
result <- data.frame(Gender = cpstarwars$gender, HAC2 = h2, Kmeans = fit$cluster) 
#Create a cross tab for HAC
result %>% group_by(HAC2) %>% select(HAC2, Gender) %>% table()
```
