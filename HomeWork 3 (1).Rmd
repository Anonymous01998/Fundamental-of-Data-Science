---
title: "HomeWork 3"
author: "Goutham Selvakumar"
date: '2022-04-26'
output: html_document
---


### PROBLEM 1

For this problem, you will perform a straightforward training and evaluation of a decision tree, as well as generate rules by hand. Load the breast_cancer_updated.csv data. These data are visual features computed from samples of breast tissue being evaluated for cancer1. As a preprocessing step, remove the IDNumber column and exclude rows with NA from the dataset. 

# Installing Packages
```{r}
library(tidyr)
library(rattle)
library(tidyverse)
library(caret)
library(dplyr)
library(rpart)
library(GGally)
library(ggplot2)
library(e1071)
# Import the Dataset
library(readxl)
setwd("C:/Users/admin/Desktop")
breast_cancer_updated <- read.csv("breast_cancer_updated.csv")
# Remove ID Number
breast_cancer_updated <- breast_cancer_updated %>% select(-c("IDNumber"))
# Remove NA rows
breast_cancer_updated <- na.omit(breast_cancer_updated)
head(breast_cancer_updated)
```

a. Apply decision tree learning (use rpart) to the data to predict breast cancer malignancy (Class) and report the accuracy using 10-fold cross validation. 
```{r}
# Evaluation method using the 10-fold cross-validation
train_control = trainControl(method = "cv", number = 10)
# Fit the Model
tree1 <- train(Class ~., data = breast_cancer_updated, method = "rpart", trControl = train_control)
tree1
```

b. Generate a visualization of the decision tree. 
```{r}
# Visualize the Decision Tree
fancyRpartPlot(tree1$finalModel, caption = "")
```
c. Generate the full set of rules using IF-THEN statements
```{r}
# n is the number of rows in the cell from 1 - 683
n = 1
for (n in 1:683)
  {
  x <- breast_cancer_updated$UniformCellSize[n]
  y <- breast_cancer_updated$UniformCellShape[n]
  
  # Print the number of the row
  print(n)
  
  # Adding the 1 to the previous n value
  n <- n+1
  if(x & y >= 2.5){
    print("malignant")
  } else if(x >= 2.5 & y < 2.5){
    print("Benign")
  }else {
    print("Benign")
  }
}
```


### PROBLEM 2

In this problem you will generate decision trees with a set of parameters. You will be using the storms data, a subset of the NOAA Atlantic hurricane database2 , which includes the positions and attributes of 198 tropical storms (potential hurricanes), measured every six hours during the lifetime of a storm. It is part of the dplyr library, so load the library and you will be able to access it. As a preprocessing step, view the data and make sure the target variable (category) is converted to a factor (as opposed to character string). 

```{r}
# View storms dataset from the dplye library
head(storms)
# Create a copy 
cpstorms <- storms
# Convert category to a factor
cpstorms$category <- as.factor(cpstorms$category)
# Check if it was done correctly 
is.factor(cpstorms$category)
# Now to remove any rows with NAs
cpstorms <- na.omit(cpstorms)
```

a. Build a decision tree using the following hyperparameters, maxdepth=2, minsplit=5 and minbucket=3. Be careful to use the right method of training so that you are not automatically tuning the cp parameter, but you are controlling the aforementioned parameters specifically. Use cross validation to report your accuracy score. These parameters will result in a relatively small tree. 
```{r}
# Set hyperparameters which controls minsplit, maxdepth, and minbucket
hypers = rpart.control(minsplit = 5, maxdepth = 2, minbucket = 3)
# Fit the Model
tree2 <- train(category ~., data = cpstorms, control = hypers, trControl = train_control, method = "rpart1SE")
# Evaluate the Model
tree2
# Visualize the Decision Tree
fancyRpartPlot(tree2$finalModel, caption = "")
```

b. To see how this performed with respect to the individual classes, we could use a confusion matrix. We also want to see if that aspect of performance is different on the train versus the test set. Create a train/test partition. Train on the training set. By making predictions with that model on the train set 
and on the test set separately, use the outputs to create two separate confusion matrices, one for each partition. Remember, we are testing if the model built with the training data performs differently on data used to train it (train set) as opposed to new data (test set). Compare the confusion matrices and report which classes it has problem classifying. Do you think that both are performing similarly and what does that suggest about overfitting for the model? 
```{r}
# I chose to partition the data into 70% train and 30% test
index = createDataPartition(y = cpstorms$category, p = 0.7, list = FALSE)
# Now set the training and test sets
# Everything in the generated index list
train_set = cpstorms[index,]
# Everything except the generated indices
test_set = cpstorms[-index,]
# Fit the Model using the training set
tree3 <- train(category ~., data = train_set, control = hyper, trControl = train_control, method = "rpart1SE")
# Training Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree3, train_set)
# Confusion Matrix
storm_train <- confusionMatrix(train_set$category, pred_tree)
storm_train
# Test Set
# Evaluate the fit with a Confusion Matrix
pred_tree <- predict(tree3, test_set)
# Confusion Matrix
storm_test <- confusionMatrix(test_set$category, pred_tree)
storm_test
# Get the training accuracy
a_train <- storm_train$overall[1]
# Get the testing accuracy
a_test <- storm_test$overall[1]
# Get the number of nodes
nodes <- nrow(tree3$finalModel$frame)
# Form the table
comp_tb1 <- data.frame("Nodes" = nodes, "TrainAccuracy" = a_train, "TestAccuracy" = a_test, "MaxDepth" = 2, "MinSplit" = 2, "Minbucket" = 3)
comp_tb1
```

### PROBLEM 3

This is will be an extension of Problem 2, using the same data and class. Here you will build many decision trees, manually tuning the parameters to gain intuition about the tradeoffs and how these tree parameters affect the complexity and quality of the model. The goal is to find the best tree model, which means it should be accurate but not too complex that the model overfits the training data. We will achieve this by using multiple sets of parameters and creating a graph of accuracy versus complexity for the training and the test sets (refer to the tutorial). This problem may require a significant amount of effort because you will need to train a substantial number of trees (at least 10). 

a. Partition your data into 80% for training and 20% for the test data set
```{r}
# Partition the data at 80% for training, which is the 0.8 here
index = createDataPartition(y = cpstorms$category, p = 0.8, list = FALSE)
# Everything in the generated index list
train_set = cpstorms[index,]
# Everything except the generated indices
test_set = cpstorms[-index,]
```

b. Train at least 10 trees using different sets of parameters, through you made need more. Create the graph described above such that you can identify the inflection point where the tree is overfitting and pick a high-quality decision tree. Your strategy should be to make at least one very simple model and 
at least one very complex model and work towards the center by changing different parameters. Generate a table that contains all of the parameters (maxdepth, minsplit, minbucket, etc) used along with the number of nodes created, and the training and testing set accuracy values. The number of rows will be equal to the number of sets of parameters used. You will use the data in the table to generate the graph. The final results to be reported for this problem are the table and graph. 
```{r}
# Initialize the Cross-Validation
train_control = trainControl(method = "cv", number = 10)

# Tree 1
hypers = rpart.control(minsplit = 2, maxdepth = 1, minbucket = 2)
tree1 <- train(category ~., data = train_set, control = hypers, trControl = train_control, method = "rpart1SE")
# Training Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree1, train_set)
# Confusion Matrix
cfm_train <- confusionMatrix(train_set$category, pred_tree)
# Test Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree1, test_set)
# Confusion Matrix
cfm_test <- confusionMatrix(test_set$category, pred_tree)
# Get Training Accuracy
a_train <- cfm_train$overall[1]
# Get Testing Accuracy
a_test <- cfm_test$overall[1]
# Get the number of nodes
nodes <- nrow(tree1$finalModel$frame)
# Form the table
comp_tb1 <- data.frame("Nodes" = nodes, "TrainAccuracy" = a_train, "TestAccuracy" = a_test, "MaxDepth" = 1, "Minsplit" = 2, "Minbucket" = 2)
# Now Repeat, 9 more times

# Tree2
hypers = rpart.control(minsplit = 5, maxdepth = 2, minbucket = 5)
tree2 <- train(category ~., data = train_set, control = hypers, trControl = train_control, method = "rpart1SE")
# Training Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree2, train_set)
# Confusion Matrix
cfm_train <- confusionMatrix(train_set$category, pred_tree)
# Test Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree2, train_set)
# Confusion Matrix
cfm_train <- confusionMatrix(test_set$category, pred_tree)
# Get Training Accuracy
a_train <- cfm_train$overall[1]
# Get Testing Accuracy
a_test <- cfm_test$overall[1]
# Get Number of Nodes
nodes <- nrow(tree2$finalModel$frame)
# Add rows to the table - Make sure the order is correct
comp_tb1 <- comp_tb1 %>% rbind(list(nodes, a_train, a_test, 2, 5, 5))

# Tree 3
hypers = rpart.control(minsplit = 50, maxdepth = 3, minbucket = 50)
tree3 <- train(category ~., data = train_set, control = hypers, trControl = train_control, method = "rpart1SE")
# Training Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree3, train_set)
# Confusion Matrix
cfm_train <- confusionMatrix(train_set$category, pred_tree)
# Test Set
# Evaluate the fit with a Confusion Matrix
pred_tree <- predict(tree3, test_set)
# Confusion Matrix
cfm_train <- confusionMatrix(test_set$category, pred_tree)
# Test Set
# Evaluate the fit with a Confusion Matrix
pred_tree <- predict(tree3, test_set)
# Confusion Matrix
cfm_test <- confusionMatrix(test_set$category, pred_tree)
# Get Training Accuracy
a_train <- cfm_train$overall[1]
# Get Testing Accuracy
a_test <- cfm_test$overall[1]
# Get Number of Nodes
nodes <- nrow(tree3$finalModel$frame)
# Add rows to the table - Make sure the order is correct
comp_tb1 <- comp_tb1 %>% rbind(list(nodes, a_train, a_test, 3, 50, 50))

# Tree 4
hypers = rpart.control(minsplit = 100, maxdepth = 4, minbucket = 100)
tree4 <- train(category ~., data = train_set, control = hypers, trControl = train_control, method = "rpart1SE")
# Training Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree4, train_set)
# Confusion Matrix
cfm_train <- confusionMatrix(train_set$category, pred_tree)
# Test Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree4, test_set)
# Confusion Matrix
cfm_test <- confusionMatrix(test_set$category, pred_tree)
# Get the training accuracy
a_train <- cfm_train$overall[1]
# Get the testing accuracy
a_test <- cfm_test$overall[1]
# Get the number of nodes
nodes <- nrow(tree4$finalModel$frame)
# Add the rows to the table - Make sure the order is correct
comp_tb1 <- comp_tb1 %>% rbind(list(nodes, a_train, a_test, 4, 100, 100))

# Tree 5
hypers = rpart.control(minspli = 1000, maxdepth = 4, minbucket = 1000)
tree5 <- train(category ~., data = train_set, control = hypers, trControl = train_control, method = "rpart1SE")
# Training Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree5, train_set)
# Confusion Matrix
cfm_train <- confusionMatrix(train_set$category, pred_tree)
# Test Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree5, test_set)
# Confusion Matrix
cfm_test <- confusionMatrix(test_set$category, pred_tree)
# Get the training accuracy
a_train <- cfm_train$overall[1]
# Get the testing accuracy
a_test <- cfm_test$overall[1]
# Get the number of nodes
nodes <- nrow(tree5$finalModel$frame)
# Add the rows to the table - Make sure the order is correct
comp_tb1 <- comp_tb1 %>% rbind(list(nodes, a_train, a_test, 4, 1000, 1000))

# Tree 6
hypers = rpart.control(minsplit = 5000, maxdepth = 8, minbucket = 5000)
tree6 <- train(category ~., data = train_set, control = hypers, trControl = train_control, method = "rpart1SE")
# Training Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree6, train_set)
# Confusion Matrix
cfm_test <- confusionMatrix(train_set$category, pred_tree)
# Test Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree6, test_set)
# Confusion Matrix
cfm_test <- confusionMatrix(test_set$category, pred_tree)
# Get the training accuracy
a_train <- cfm_train$overall[1]
# Get the testing accuracy
a_test <- cfm_test$overall[1]
# Get the number of nodes
nodes <- nrow(tree6$finalModel$frame)
# Add the rows to the table - Make sure the order is correct
comp_tb1 <- comp_tb1 %>% rbind(list(nodes, a_train, a_test, 8, 5000, 5000))

# Tree 7 
hypers = rpart.control(minsplit = 10000, maxdepth = 25, minbucket = 5000)
tree7 <- train(category ~., data = train_set, control = hypers, trControl = train_control, method = "rpart1SE")
# Training Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree7, train_set)
# Confusion Matrix
cfm_train <- confusionMatrix(train_set$category, pred_tree)
# Test Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree7, test_set)
# Confusion Matrix
cfm_test <- confusionMatrix(test_set$category, pred_tree)
# Get the training accuracy
a_train <- cfm_train$overall[1]
# Get the testing accuracy
a_test <- cfm_test$overall[1]
# Get the number of nodes
nodes <- nrow(tree7$finalModel$frame)
# Add rows to the table - Make sure the order is correct
comp_tb1 <- comp_tb1 %>% rbind(list(nodes, a_train, a_test, 25, 10000, 10000))

# Tree 8 
hypers = rpart.control(minsplit = 25000, maxdepth = 20, minbucket = 25000)
tree8 <- train(category ~., data = train_set, control = hypers, trControl = train_control, method = "rpart1SE")
# Training Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree8, train_set)
# Confusion Matrix
cfm_test <- confusionMatrix(train_set$category, pred_tree)
# Test Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree8, test_set)
# Confusion Matrix
cfm_test <- confusionMatrix(test_set$category, pred_tree)
# Get the training accuracy
a_train <- cfm_train$overall[1]
# Get the testing accuracy
a_test <- cfm_test$overall[1]
# Get the number of nodes
nodes <- nrow(tree8$finalModel$frame)
# Add rows to the table - Make sure the order is correct
comp_tb1 <- comp_tb1 %>% rbind(list(nodes, a_train, a_test, 20, 25000, 25000))

# Tree 9 
hypers = rpart.control(minsplit = 50000, maxdepth = 25, minbucket = 50000)
tree9 <- train(category ~., data = train_set, control = hypers, trControl = train_control, method = "rpart1SE")
# Training Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree9, train_set)
# Confusion Matrix
cfm_train <- confusionMatrix(train_set$category, pred_tree)
# Test Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree9, test_set)
# Confusion Matrix
cfm_test <- confusionMatrix(test_set$category, pred_tree)
# Get the training accuracy
a_train <- cfm_train$overall[1]
# Get the testing accuracy
a_test <- cfm_test$overall[1]
# Get the number of nodes
nodes <- nrow(tree9$finalModel$frame)
# Add the rows to the table - Make sure the order is correct
comp_tb1 <- comp_tb1 %>% rbind(list(nodes, a_train, a_test, 25, 50000, 50000))

# Tree 10
hypers = rpart.control(minsplit = 75000, maxdepth = 30, minbucket = 75000)
tree10 <- train(category ~., data = train_set, control = hypers, trControl = train_control, method = "rpart1SE")
# Training Set
#Evaluate the fit with a confusion matrix
pred_tree <- predict(tree10, train_set)
# Confusion Matrix
cfm_train <- confusionMatrix(train_set$category, pred_tree)
# Test Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree10, test_set)
# Confusion Matrix
cfm_train <- confusionMatrix(test_set$category, pred_tree)
# Get the training accuracy
a_train <- cfm_train$overall[1]
# Get the testing accuracy
a_test <- cfm_test$overall[1]
# Get the number of nodes
nodes <- nrow(tree10$finalModel$frame)
# Add the rows to the table - Make sure the order is correct
comp_tb1 <- comp_tb1 %>% rbind(list(nodes, a_train, a_test, 50, 75000, 75000))
comp_tb1
# Visualize with the scatter plot
ggplot(comp_tb1, aes(x = nodes)) +
  geom_point(aes(y = TrainAccuracy), color = "red") +
  geom_point(aes(y = TestAccuracy), color = "blue") +
  ylab("Accuracy")
```

c. Identify the final choice of model, list it parameters and evaluate with a the confusion matrix to make sure that it gets balanced performance over classes. Also get a better accuracy estimate for this tree using cross validation. 
```{r}
hypers = rpart.control(minsplit = 5, maxdepth = 2, minbucket = 5)
tree2 <- train(category ~., data = train_set, control = hypers, trControl = train_control, method = "rpart1SE")
# Training Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree2, train_set)
# Confusion Matrix
cfm_train <- confusionMatrix(train_set$category, pred_tree)
# Test Set
# Evaluate the fit with a confusion matrix
pred_tree <- predict(tree2, test_set)
# Confusion Matrix
cfm_test <- confusionMatrix(test_set$category, pred_tree)
cfm_train
```
### PROBLEM 4

In this problem you will identify the most important independent variables used in a classification model. Use the Bank_Modified.csv data. As a preprocessing step, remove the ID column and make sure to convert the target variable, approval, from a string to a factor. 

```{r}
# Import dataset
library(readxl)
Bank_Modified <- read_csv("C:/Users/admin/Desktop/Bank_Modified.csv")
# Remove ID column
Bank_Modified <- Bank_Modified %>% select(-c("ID"))
# Convert approval to factor
Bank_Modified$approval <- as.factor(Bank_Modified$approval)
# Remove NA's
Bank_Modified <- na.omit(Bank_Modified)
head(Bank_Modified)
```

a. Build your initial decision tree model with minsplit=10 and maxdepth=20
```{r}
# Set the hyperparameters
hypers = rpart.control(minsplit = 10, maxdepth = 20)
# Fit the Model
tree1 <- train(approval ~., data = Bank_Modified, control = hypers, trControl = train_control, method ="rpart1SE")
# Visualize the decision tree
fancyRpartPlot(tree1$finalModel, caption = "")
```

b. Run variable importance analysis on the model and print the result.
```{r}
# Fit the Model
tree1 <- train(approval ~., data = Bank_Modified, method = "rpart1SE", trControl = train_control)
# View the variable importance scores using the varImp function
var_imp <- varImp(tree1, scale = FALSE)
print(var_imp)
```

c.  Generate a plot to visualize the variables by importance.
```{r}
plot(var_imp)
```

d. Rebuild your model with the top six variables only, based on the variable relevance analysis. Did this change have an effect on the accuracy?
```{r}
# Make a copy with approval and 6 top predictors
newBank_Modified <- Bank_Modified %>% select(c("approval", "bool1", "cont4", "bool2", "cont6", "ages", "cont2"))
head(newBank_Modified)
# Partition into training and test sets
index = createDataPartition(y = newBank_Modified$approval, p=0.7, list = FALSE)
train_set = newBank_Modified[index,]
test_set = newBank_Modified[-index,]
tree2 <- train(approval ~., data = train_set, method = "rpart1SE", trControl = train_control)
# View the variable importance scores using the varImp function
var_imp <- varImp(tree2, scale = FALSE)
print(var_imp)
plot(var_imp)
tree1
tree2
```
e. Visualize the trees from (a) and (d) and report if reducing the number of variables had an effect on the size of the tree?
```{r}
fancyRpartPlot(tree1$finalModel, caption = "")
fancyRpartPlot(tree2$finalModel, caption = "")
```

